This project is my coursework on sequential analysis while doing my Masters at City, University of London. It creates a seq2seq based chatbot using Gated Recurrent Units (GRU).
It looks the examine the impact of the following:

  GRU Tuning:
  
      GRU Embedding Size
    
      Number of GRU Layers
    
      GRU Dropout
    
      Teacher Forcing
    
      Use of Bidirectional Encoders
    
      Max Sequence Length

  Seq2Seq Tuning:
  
      Comparison of LSTM vs GRU
      
      Use of Bahdanau and Luong Attention
    
  

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "547b2e76-ca81-4af3-b7b5-be9b71c5657f",
   "metadata": {},
   "source": [
    "<h2>Imports</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b28db960-eaf7-4621-87ce-379476655879",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/darylramdin/opt/anaconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from corpus import Corpus, CornellMovieCorpus, Vocabulary\n",
    "from rnn import Encoder, Decoder\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "import random\n",
    "from torch.utils.data import DataLoader as DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3837ad4f-cc3d-44ae-a4b3-fd1ac9dfa6d8",
   "metadata": {},
   "source": [
    "<h2>Parameter Settings</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5da89873-4a1f-466c-8773-e724b424627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(77)\n",
    "SIZEOF_EMBEDDING = 256\n",
    "NUMBER_OF_EPOCHS = 1\n",
    "PRINT_INTERVAL = 10\n",
    "BATCH_SIZE = 1\n",
    "TEACHER_FORCING = 0\n",
    "TEACHER_FORCING_DECAY = 0\n",
    "PROGRESS_INTERVAL = 100\n",
    "LEARNING_RATE = 1e-03\n",
    "CONVO_MODE = Corpus.FULL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b536192-9da1-44a0-8f42-543ed4229f07",
   "metadata": {},
   "source": [
    "<h2>Use the GPU if present</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cadd0113-1b64-487c-a2ee-6d2ed50c1bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('mps')\n",
    "if (torch.cuda.is_available()):\n",
    "   device = torch.device('cuda')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eefd147-e96b-4aa0-922f-ca1cb8a51824",
   "metadata": {},
   "source": [
    "<h2>Create a Cornell Movie Corpus </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c057d9c-517b-4e05-8236-b812729d0629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading movie lines...\n",
      "Creating vocabulary...\n",
      "Converting conversation line numbers to text...\n",
      "Creating conversation chain\n"
     ]
    }
   ],
   "source": [
    "corpus = CornellMovieCorpus(convo_mode=CONVO_MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdee165-0686-4d27-ae09-10a55d8010fc",
   "metadata": {},
   "source": [
    "<h2>Let's look at some data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dc7982e-be6e-44cc-b844-af30a8872892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304713 movie lines loaded\n",
      "257093 distinct movie lines exist\n",
      "83097 conversations loaded\n"
     ]
    }
   ],
   "source": [
    "lines = list(corpus.movie_lines.items())\n",
    "print(len(lines), \"movie lines loaded\")\n",
    "\n",
    "distinct_lines = [line[1][\"prepped_text\"] for line in lines]\n",
    "print(len(set(distinct_lines)), \"distinct movie lines exist\")\n",
    "\n",
    "print(len(corpus.movie_convos), \"conversations loaded\")                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fea16e0-7ecc-43dd-bd45-577127d2c424",
   "metadata": {},
   "source": [
    "<h2>Create Encoders and Decoders</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76806262-be72-4c3a-8c16-9dad85c1e55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): Embedding(49048, 256)\n",
       "  (relu): ReLU()\n",
       "  (gru): GRU(256, 256, batch_first=True)\n",
       "  (out): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (out2): Linear(in_features=128, out_features=49048, bias=True)\n",
       "  (softmax): LogSoftmax(dim=2)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizeof_vocab = corpus.vocabulary.len\n",
    "\n",
    "encoder = Encoder(sizeof_vocab, SIZEOF_EMBEDDING)\n",
    "decoder = Decoder(SIZEOF_EMBEDDING, sizeof_vocab)\n",
    "\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5351b6-6a04-4b55-bde9-1de7cad34fd8",
   "metadata": {},
   "source": [
    "<h2>Let's setup our trainer</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "365a2fdb-c6fb-4a55-81e0-0ed62e58f8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_optimizer = optim.Adam(encoder.parameters(),lr=LEARNING_RATE)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(),lr=LEARNING_RATE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "dataloader = DataLoader(corpus, batch_size=BATCH_SIZE,shuffle=True)\n",
    "\n",
    "#Let's get a random exchange pair\n",
    "epoch_loss = 0\n",
    "start_time = datetime.now()\n",
    "training_loss = []\n",
    "total_sequences = 0\n",
    "interval_sequences = 0\n",
    "\n",
    "if corpus.convo_mode==corpus.FULL:\n",
    "    batch_type = \"Conversation\"\n",
    "else:\n",
    "    batch_type = \"Exchange Pair\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952a3f48-e3ac-4a59-9b5d-a44e2e47dacb",
   "metadata": {},
   "source": [
    "<h2>Train</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65c1dff4-b1e7-4805-9eb1-e5010226720a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 32\u001b[0m\n\u001b[1;32m     27\u001b[0m total_sequences\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39minterval_sequences\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#print(\"Tensor shapes\", Q_tensors.shape, A_tensors.shape)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#Encode the batch\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m encoder_output, encoder_hidden \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ_tensors\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# encoder_output: (batch_size, max_seq_len, hidden_size), encoder_hidden: (1, batch_size, hidden_size)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# #The decoder accepts an input and the previous hidden start\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# At the start, the first input is the SOS token and the\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# hidden state is the output of the encoder i.e. context vector\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#The first input to the decoder is the SOS token.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#Iterate through the batch of A_tensors and get the first element in each sequence\u001b[39;00m\n\u001b[1;32m     40\u001b[0m decoder_input \u001b[38;5;241m=\u001b[39m A_tensors[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#decoder_input (batch_size, 1)\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-City,UniversityofLondon/Documents/INM706 - Deep Learning for Sequential Analysis/Coursework/inm706-coursework-daryl-ramdin/rnn.py:20\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, input_tensor)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,input_tensor):\n\u001b[0;32m---> 20\u001b[0m     output_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     output_tensor, hidden_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgru(output_tensor)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output_tensor, hidden_tensor\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/sparse.py:160\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUMBER_OF_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    processed_total_batches = 0\n",
    "    batch_counter = 0\n",
    "\n",
    "    interval_loss = 0\n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        \n",
    "        teacher_forcing = False\n",
    "        \n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        # coder_optimizer.zero_grad()\n",
    "\n",
    "        Q_tensors = batch[1][\"Q\"] #shape(batch_size,convo_length,seq_len)\n",
    "        A_tensors= batch[1][\"Q\"] #shape(batch_size,convo_length,seq_len)\n",
    "        \n",
    "        Q_tensors.to(device)\n",
    "        A_tensors.to(device)\n",
    "\n",
    "        Q_tensors = Q_tensors.squeeze(0)\n",
    "        A_tensors = A_tensors.squeeze(0)\n",
    "        \n",
    "        seq_length= A_tensors.shape[1]\n",
    "\n",
    "        interval_sequences+=len(A_tensors)\n",
    "        total_sequences+=interval_sequences\n",
    "\n",
    "        #print(\"Tensor shapes\", Q_tensors.shape, A_tensors.shape)\n",
    "\n",
    "        #Encode the batch\n",
    "        encoder_output, encoder_hidden = encoder(Q_tensors) # encoder_output: (batch_size, max_seq_len, hidden_size), encoder_hidden: (1, batch_size, hidden_size)\n",
    "\n",
    "        # #The decoder accepts an input and the previous hidden start\n",
    "        # At the start, the first input is the SOS token and the\n",
    "        # hidden state is the output of the encoder i.e. context vector\n",
    "\n",
    "        #The first input to the decoder is the SOS token.\n",
    "        #Iterate through the batch of A_tensors and get the first element in each sequence\n",
    "        decoder_input = A_tensors[:, 0].view(-1,1) #decoder_input (batch_size, 1)\n",
    "\n",
    "        #The initial hidden input to the decoder is the last hidden of the encoder.\n",
    "        #For each sequence in the encoder hidden batch, get the last token. encoder_hidden: (batch_size, max_seq_len, hidden_size)\n",
    "        # decoder_hidden = encoder_output[:,-1:,:].squeeze(1) \n",
    "        # decoder_hidden = decoder_hidden.unsqueeze(0) #decoder_hidden  (1, batch_size, sizeof_hidden)\n",
    "        decoder_hidden = encoder_hidden\n",
    "            \n",
    "#         #Get the output from the decoder.\n",
    "#         #decoder_input (batch_size,1), decoder_hidden (1, batch_size, sizeof_hidden)\n",
    "#         decoder_output, decoder_hidden = decoder(decoder_input,decoder_hidden) #decoder_output (batch_size, 1, sizeof_vocab), decoder_hidden (batch_size, sizeof_hidden)\n",
    "#         decoder_output = decoer_output.squeeze(1)\n",
    "        \n",
    "#         #calculate the loss by comparing with the ouput with the first non-SOS token in the A tensor\n",
    "#         target = A_tensors[:,1]\n",
    "#         loss = criterion(decoder_output,target)\n",
    "\n",
    "#         #Get the top prediction indices\n",
    "#         decoder_output = decoder_output.topk(k=1, dim=1).indices\n",
    "        \n",
    "#         #  For each batch, we now iterate through the rest of the sequence\n",
    "#         # in each A_tensor decoding outputs and hidden states\n",
    "        \n",
    "        loss = 0\n",
    "        #If we are using teach forcing then the decoder_hidden is the \n",
    "        if random.random() < TEACHER_FORCING: teacher_forcing = True\n",
    "        \n",
    "        for i in range(seq_length-1):\n",
    "            \n",
    "            #if teacher forcing then the target is fed in as the input\n",
    "            if teacher_forcing: decoder_input = A_tensor[:,i+1]\n",
    "            \n",
    "            #Get the decoder output and hidden state for the\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden) #decoder_output (batch_size, 1, sizeof_vocab), decoder_hidden (batch_size, sizeof_hidden)\n",
    "\n",
    "            decoder_output = decoder_output.squeeze(1)\n",
    "            # calculate the loss by comparing with the tensor of the target\n",
    "            target = A_tensors[:, i+1]\n",
    "            loss += criterion(decoder_output, target)\n",
    "\n",
    "            #Get the top prediction for each batch\n",
    "            decoder_input = decoder_output.topk(k=1, dim=1).indices\n",
    "\n",
    "        convo_loss = loss/corpus.max_seq_length\n",
    "        interval_loss += convo_loss\n",
    "\n",
    "\n",
    "        if batch_counter%PROGRESS_INTERVAL == 0 and batch_counter:\n",
    "            #Calculate the average sequence loss over the interval\n",
    "            end_time = datetime.now()\n",
    "            timediff = end_time - start_time\n",
    "            timediff = timediff.seconds\n",
    "            print(batch_type + ' Number: {0:1d}, Number of sequences: {1:1d}, Average sequence loss {2:.6f}, in {3:d} seconds'.format(batch_counter,interval_sequences,interval_loss/PROGRESS_INTERVAL,timediff))\n",
    "            start_time = datetime.now()\n",
    "            interval_loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            interval_loss = 0\n",
    "            interval_sequences = 0\n",
    "\n",
    "        batch_counter+=1\n",
    "        training_loss.append([batch_counter,convo_loss.item()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ff2e84-735a-4cdb-8831-dcb3e08d3fad",
   "metadata": {},
   "source": [
    "<h2>Print the results</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2395f8-d7c1-4c04-9ca7-268c68a3ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = np.array(training_loss)\n",
    "plt.plot(training_loss[:,0][::1], training_loss[:,1][::1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1fe0a4-6fee-453a-ace7-3640f518f9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "print(random.random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274f0f65-8b72-4954-b620-d6fc841c15ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

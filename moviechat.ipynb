{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "547b2e76-ca81-4af3-b7b5-be9b71c5657f",
   "metadata": {},
   "source": [
    "<h2>Imports</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28db960-eaf7-4621-87ce-379476655879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from corpus import Corpus, CornellMovieCorpus, Vocabulary\n",
    "from rnn import Encoder, Decoder\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "import random\n",
    "from torch.utils.data import DataLoader as DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3837ad4f-cc3d-44ae-a4b3-fd1ac9dfa6d8",
   "metadata": {},
   "source": [
    "<h2>Parameter Settings</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da89873-4a1f-466c-8773-e724b424627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(77)\n",
    "SIZEOF_EMBEDDING = 256\n",
    "NUMBER_OF_EPOCHS = 1\n",
    "PRINT_INTERVAL = 10\n",
    "BATCH_SIZE = 1\n",
    "TEACHER_FORCING = 0\n",
    "TEACHER_FORCING_DECAY = 0\n",
    "PROGRESS_INTERVAL = 100\n",
    "LEARNING_RATE = 1e-03\n",
    "CONVO_MODE == Corpus.FULL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b536192-9da1-44a0-8f42-543ed4229f07",
   "metadata": {},
   "source": [
    "<h2>Use the GPU if present</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadd0113-1b64-487c-a2ee-6d2ed50c1bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps')\n",
    "if (torch.cuda.is_available()):\n",
    "   device = torch.device('cuda')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eefd147-e96b-4aa0-922f-ca1cb8a51824",
   "metadata": {},
   "source": [
    "<h2>Create a Cornell Movie Corpus </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c057d9c-517b-4e05-8236-b812729d0629",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = CornellMovieCorpus(convo_mode=convo_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdee165-0686-4d27-ae09-10a55d8010fc",
   "metadata": {},
   "source": [
    "<h2>Let's look at some data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc7982e-be6e-44cc-b844-af30a8872892",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = list(corpus.movie_lines.items())\n",
    "print(len(lines), \"movie lines loaded\")\n",
    "\n",
    "distinct_lines = [line[1][\"prepped_text\"] for line in lines]\n",
    "print(len(set(distinct_lines)), \"distinct movie lines exist\")\n",
    "\n",
    "print(len(corpus.movie_convos), \"conversations loaded\")                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fea16e0-7ecc-43dd-bd45-577127d2c424",
   "metadata": {},
   "source": [
    "<h2>Create Encoders and Decoders</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76806262-be72-4c3a-8c16-9dad85c1e55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizeof_vocab = corpus.vocabulary.len\n",
    "\n",
    "encoder = Encoder(sizeof_vocab, SIZEOF_EMBEDDING)\n",
    "decoder = Decoder(SIZEOF_EMBEDDING, sizeof_vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5351b6-6a04-4b55-bde9-1de7cad34fd8",
   "metadata": {},
   "source": [
    "<h2>Let's setup our trainer</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a2fdb-c6fb-4a55-81e0-0ed62e58f8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_optimizer = optim.Adam(encoder.parameters(),lr=LEARNING_RATE)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(),lr=LEARNING_RATE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "dataloader = DataLoader(corpus, batch_size=BATCH_SIZE,shuffle=True)\n",
    "\n",
    "#Let's get a random exchange pair\n",
    "epoch_loss = 0\n",
    "start_time = datetime.now()\n",
    "training_loss = []\n",
    "total_sequences = 0\n",
    "interval_sequences = 0\n",
    "\n",
    "if corpus.convo_mode==corpus.FULL:\n",
    "    batch_type = \"Conversation\"\n",
    "else:\n",
    "    batch_type = \"Exchange Pair\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952a3f48-e3ac-4a59-9b5d-a44e2e47dacb",
   "metadata": {},
   "source": [
    "<h2>Train</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c1dff4-b1e7-4805-9eb1-e5010226720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(number_of_epochs):\n",
    "    epoch_loss = 0\n",
    "    processed_total_batches = 0\n",
    "    batch_counter = 0\n",
    "\n",
    "    interval_loss = 0\n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        \n",
    "        teach_forcing = False\n",
    "        \n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        # coder_optimizer.zero_grad()\n",
    "\n",
    "        Q_tensors = batch[1][\"Q\"] #shape(batch_size,convo_length,seq_len)\n",
    "        A_tensors= batch[1][\"Q\"] #shape(batch_size,convo_length,seq_len)\n",
    "\n",
    "        Q_tensors = Q_tensors.squeeze(0)\n",
    "        A_tensors = A_tensors.squeeze(0)\n",
    "        \n",
    "        seq_length= A_tensors.shape()[1]\n",
    "\n",
    "        interval_sequences+=len(A_tensors)\n",
    "        total_sequences+=interval_sequences\n",
    "\n",
    "        #print(\"Tensor shapes\", Q_tensors.shape, A_tensors.shape)\n",
    "\n",
    "        #Encode the batch\n",
    "        encoder_output, encoder_hidden = encoder(Q_tensors) # encoder_output: (batch_size, max_seq_len, hidden_size), encoder_hidden: (1, batch_size, hidden_size)\n",
    "\n",
    "        # #The decoder accepts an input and the previous hidden start\n",
    "        # At the start, the first input is the SOS token and the\n",
    "        # hidden state is the output of the encoder i.e. context vector\n",
    "\n",
    "        #The first input to the decoder is the SOS token.\n",
    "        #Iterate through the batch of A_tensors and get the first element in each sequence\n",
    "        decoder_input = A_tensors[:, 0].view(-1,1) #decoder_input (batch_size, 1)\n",
    "\n",
    "        #The initial hidden input to the decoder is the last hidden of the encoder.\n",
    "        #For each sequence in the encoder hidden batch, get the last token. encoder_hidden: (batch_size, max_seq_len, hidden_size)\n",
    "        # decoder_hidden = encoder_output[:,-1:,:].squeeze(1) \n",
    "        # decoder_hidden = decoder_hidden.unsqueeze(0) #decoder_hidden  (1, batch_size, sizeof_hidden)\n",
    "        decoder_hidden = encoder_hidden\n",
    "            \n",
    "#         #Get the output from the decoder.\n",
    "#         #decoder_input (batch_size,1), decoder_hidden (1, batch_size, sizeof_hidden)\n",
    "#         decoder_output, decoder_hidden = decoder(decoder_input,decoder_hidden) #decoder_output (batch_size, 1, sizeof_vocab), decoder_hidden (batch_size, sizeof_hidden)\n",
    "#         decoder_output = decoer_output.squeeze(1)\n",
    "        \n",
    "#         #calculate the loss by comparing with the ouput with the first non-SOS token in the A tensor\n",
    "#         target = A_tensors[:,1]\n",
    "#         loss = criterion(decoder_output,target)\n",
    "\n",
    "#         #Get the top prediction indices\n",
    "#         decoder_output = decoder_output.topk(k=1, dim=1).indices\n",
    "        \n",
    "#         #  For each batch, we now iterate through the rest of the sequence\n",
    "#         # in each A_tensor decoding outputs and hidden states\n",
    "        \n",
    "        #If we are using teach forcing then the decoder_hidden is the \n",
    "        if random.random() < TEACHER_FORCING: teacher_forcing = True\n",
    "        \n",
    "        for i in range(seq_length):\n",
    "            \n",
    "            #if teacher forcing then the target is fed in as the input\n",
    "            if teacher_forcing: decoder_input = A_tensor[:,i+1]\n",
    "            \n",
    "            #Get the decoder output and hidden state for the\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden) #decoder_output (batch_size, 1, sizeof_vocab), decoder_hidden (batch_size, sizeof_hidden)\n",
    "\n",
    "            decoder_output = decoder_output.squeeze(1)\n",
    "            # calculate the loss by comparing with the tensor of the target\n",
    "            target = A_tensors[:, i+1]\n",
    "            loss += criterion(decoder_output, target)\n",
    "\n",
    "            #Get the top prediction for each batch\n",
    "            decoder_input = decoder_output.topk(k=1, dim=1).indices\n",
    "\n",
    "        convo_loss = loss/corpus.max_seq_length\n",
    "        interval_loss += convo_loss\n",
    "\n",
    "\n",
    "        if batch_counter%progress_interval == 0 and batch_counter:\n",
    "            #Calculate the average sequence loss over the interval\n",
    "            end_time = datetime.now()\n",
    "            timediff = end_time - start_time\n",
    "            timediff = timediff.seconds\n",
    "            print(batch_type + ' Number: {0:1d}, Number of sequences: {1:1d}, Average sequence loss {2:.6f}, in {3:d} seconds'.format(batch_counter,interval_sequences,interval_loss/progress_interval,timediff))\n",
    "            start_time = datetime.now()\n",
    "            interval_loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            interval_loss = 0\n",
    "            interval_sequences = 0\n",
    "\n",
    "        batch_counter+=1\n",
    "        training_loss.append([batch_counter,convo_loss.item()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ff2e84-735a-4cdb-8831-dcb3e08d3fad",
   "metadata": {},
   "source": [
    "<h2>Print the results</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2395f8-d7c1-4c04-9ca7-268c68a3ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = np.array(training_loss)\n",
    "plt.plot(training_loss[:,0][::1], training_loss[:,1][::1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a1fe0a4-6fee-453a-ace7-3640f518f9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35985499106321084\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "print(random.random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274f0f65-8b72-4954-b620-d6fc841c15ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
